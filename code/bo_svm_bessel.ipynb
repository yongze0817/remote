{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from skopt import gp_minimize\n",
    "from skopt.learning import RandomForestRegressor\n",
    "from skopt.space import Real, Categorical, Space\n",
    "from skopt.utils import use_named_args\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "import json\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\"自定义JSON编码器，用于处理numpy数据类型\"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NumpyEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_ess(Y, W, X, alpha, support_indices):\n",
    "        weights = np.zeros(X.shape[0])\n",
    "        weights[support_indices] = np.abs(alpha)\n",
    "        alpha = weights / np.sum(weights)\n",
    "        \n",
    "        treated_ess = (np.sum(alpha[W == 1]))**2 / np.sum(alpha[W == 1]**2)\n",
    "        control_ess = (np.sum(alpha[W == -1]))**2 / np.sum(alpha[W == -1]**2)\n",
    "\n",
    "        return treated_ess + control_ess\n",
    "    \n",
    "def estimate_diff(Y, W, X, alpha, support_indices):\n",
    "\n",
    "    # 计算权重\n",
    "    weights = np.zeros(X.shape[0])\n",
    "    weights[support_indices] = np.abs(alpha)\n",
    "    alpha = weights / np.sum(weights)\n",
    "    \n",
    "    treated_mean = np.mean(Y[W == 1])\n",
    "    control_mean = np.mean(Y[W == -1])\n",
    "    \n",
    "    treated_std = np.std(Y[W == 1], ddof=1)\n",
    "    control_std = np.std(Y[W == -1], ddof=1)\n",
    "\n",
    "    normed_diff = np.abs(treated_mean - control_mean) / np.sqrt((treated_std**2 + control_std**2) / 2)\n",
    "\n",
    "    return normed_diff\n",
    "\n",
    "def estimate_ate(Y, W, X, alpha, support_indices):\n",
    "    \n",
    "    # 计算权重\n",
    "    weights = np.zeros(X.shape[0])\n",
    "    weights[support_indices] = np.abs(alpha)\n",
    "    alpha = weights / np.sum(weights)\n",
    "    \n",
    "    treated_mean = np.sum(Y[W == 1] * alpha[W == 1]) / np.sum(alpha[W == 1])\n",
    "    control_mean = np.sum(Y[W == -1] * alpha[W == -1]) / np.sum(alpha[W == -1])\n",
    "    return treated_mean - control_mean\n",
    "\n",
    "\n",
    "\n",
    "def compute_wnayman(Y_obs, W, X, alpha, support_indices):\n",
    "    \n",
    "    # 计算权重\n",
    "    weights = np.zeros(X.shape[0])\n",
    "    weights[support_indices] = np.abs(alpha)\n",
    "    lambda_i = weights / np.sum(weights)\n",
    "    \n",
    "    # 计算mu_c(x)和mu_t(x)，这里需要根据具体数据进行定义\n",
    "    mu_c = np.mean(Y_obs[W == -1])\n",
    "    mu_t = np.mean(Y_obs[W == 1])\n",
    "\n",
    "    # 计算sigma_c^2(x)和sigma_t^2(x)，这里需要根据具体数据进行定义\n",
    "    sigma_c_squared = np.var(Y_obs[W == -1])\n",
    "    sigma_t_squared = np.var(Y_obs[W == 1])\n",
    "\n",
    "    # 计算mu_i和sigma_i^2\n",
    "    mu_i = np.where(W == -1, mu_c, mu_t)\n",
    "    sigma_i_squared = np.where(W == -1, sigma_c_squared, sigma_t_squared)\n",
    "\n",
    "    N_t = np.sum(W == 1)\n",
    "    N_c = np.sum(W == -1)\n",
    "\n",
    "    # 计算条件抽样方差\n",
    "    conditional_variance = np.sum(lambda_i[W == 1]**2 * sigma_i_squared[W == 1]) + np.sum(lambda_i[W == -1]**2 * sigma_i_squared[W == -1])\n",
    "    \n",
    "    stderr = np.sqrt(conditional_variance)\n",
    "\n",
    "\n",
    "    print(\"条件抽样方差:\", conditional_variance)\n",
    "    return stderr\n",
    "\n",
    "class ConvergenceStopper:\n",
    "    def __init__(self, patience=10, tol=1e-6):\n",
    "        self.patience = patience\n",
    "        self.tol = tol\n",
    "        self.best_value = np.inf\n",
    "        self.no_improvement = 0\n",
    "    \n",
    "    def __call__(self, result):\n",
    "        if len(result.x_iters) < 10:  # 至少10个点才开始检查\n",
    "            return False\n",
    "        \n",
    "        current_min = np.min(result.func_vals)\n",
    "        \n",
    "        if self.best_value - current_min > self.tol:\n",
    "            self.best_value = current_min\n",
    "            self.no_improvement = 0\n",
    "        else:\n",
    "            self.no_improvement += 1\n",
    "            \n",
    "        return self.no_improvement >= self.patience\n",
    "\n",
    "class BOStopper:\n",
    "    def __init__(self, conv_patience=10, conv_tol=1e-6,\n",
    "                 ci_width=0.2, ci_patience=10, max_iter=100):\n",
    "        self.conv_stopper = ConvergenceStopper(conv_patience, conv_tol)\n",
    "        # self.ci_stopper = ConfidenceIntervalStopper(ci_width, ci_patience)\n",
    "        self.max_iter = max_iter\n",
    "    \n",
    "    def __call__(self, result):\n",
    "        # 最大迭代次数检查\n",
    "        if len(result.x_iters) >= self.max_iter:\n",
    "            return True\n",
    "            \n",
    "        # 收敛检查\n",
    "        if self.conv_stopper(result):\n",
    "            return True\n",
    "        \n",
    "        # 置信区间检查\n",
    "        # if self.ci_stopper(result):\n",
    "        #     return True\n",
    "            \n",
    "        return False\n",
    "\n",
    "from scipy.special import jv\n",
    "# 定义贝塞尔核函数\n",
    "def bessel_kernel(x, y, sigma=1.0, v=1.0):\n",
    "    norm = np.linalg.norm(x - y)\n",
    "    if norm == 0:\n",
    "        return 1.0\n",
    "    return jv(v + 1, sigma * norm) / (norm ** (-v - 1))\n",
    "\n",
    "# 自定义核函数的矩阵计算\n",
    "def bessel_kernel_matrix(X, Y, sigma=1.0, v=1.0):\n",
    "    X = np.atleast_2d(X)  # 确保 X 是二维数组\n",
    "    Y = np.atleast_2d(Y)  # 确保 Y 是二维数组\n",
    "    n_samples_X, n_features = X.shape\n",
    "    n_samples_Y, _ = Y.shape\n",
    "    K = np.zeros((n_samples_X, n_samples_Y))\n",
    "    for i in range(n_samples_X):\n",
    "        for j in range(n_samples_Y):\n",
    "            K[i, j] = bessel_kernel(X[i], Y[j], sigma, v)\n",
    "    return K\n",
    "\n",
    "# from multiprocessing import Pool\n",
    "# def compute_kernel_row(i, X, Y, sigma, v):\n",
    "#     row = np.zeros(len(Y))\n",
    "#     for j in range(len(Y)):\n",
    "#         row[j] = bessel_kernel(X[i], Y[j], sigma, v)\n",
    "#     return row\n",
    "\n",
    "# def bessel_kernel_matrix_parallel(X, Y, sigma=1.0, v=1.0):\n",
    "#     X = np.atleast_2d(X)\n",
    "#     Y = np.atleast_2d(Y)\n",
    "#     n_samples_X, n_samples_Y = len(X), len(Y)\n",
    "#     K = np.zeros((n_samples_X, n_samples_Y))\n",
    "    \n",
    "#     with Pool(processes=4) as pool:\n",
    "#         results = pool.starmap(compute_kernel_row, [(i, X, Y, sigma, v) for i in range(n_samples_X)])\n",
    "    \n",
    "#     for i, row in enumerate(results):\n",
    "#         K[i] = row\n",
    "#     return K\n",
    "\n",
    "def bayesian_opt(Y, X, W, kernel, c_upper): \n",
    "    # Define the hyperparameter space\n",
    "    dim_C = Real(low=1e-4, high=c_upper, prior='log-uniform', name='C')\n",
    "    dim_class_weight_1 = Real(low=0.1, high=2.0, prior='uniform', name='class_weight_1')\n",
    "    dim_class_weight_neg1 = Real(low=0.1, high=2.0, prior='uniform', name='class_weight_neg1')\n",
    "    # gamma = Real(low=1e-4, high=1e1, prior='log-uniform', name='gamma')\n",
    "\n",
    "    dimensions = [dim_C]\n",
    "\n",
    "    # Define the target score function\n",
    "    @use_named_args(dimensions=dimensions)\n",
    "    def objective_1(**params):\n",
    "        #class_weight = {1: params['class_weight_1'], -1: params['class_weight_neg1']}\n",
    "        clf = SVC(C=params['C'], kernel=lambda X, Y: bessel_kernel_matrix(X, Y, sigma=1.0, v=1.0))\n",
    "        \n",
    "        # return score  # We negate because we want to maximize accuracy\n",
    "    \n",
    "        clf.fit(X_train, W_train)\n",
    "        score = clf.score(X_test, W_test)\n",
    "        # ess = estimate_ess(Y, W, X, alpha, support_indices)\n",
    "        return -score\n",
    "    \n",
    "    @use_named_args(dimensions=dimensions)\n",
    "    def objective_2(**params):\n",
    "        #class_weight = {1: params['class_weight_1'], -1: params['class_weight_neg1']}\n",
    "        clf = SVC(C=params['C'], kernel=lambda X, Y: bessel_kernel_matrix(X, Y, sigma=1.0, v=1.0))\n",
    "        \n",
    "        # return score  # We negate because we want to maximize accuracy\n",
    "    \n",
    "        clf.fit(X_test, W_test)\n",
    "        score = clf.score(X_train, W_train)\n",
    "        # ess = estimate_ess(Y, W, X, alpha, support_indices)\n",
    "        return -score\n",
    "\n",
    "    \n",
    "\n",
    "    #print(result)\n",
    "    # Extract optimized hyperparameters\n",
    "    def crossfitting(result, Y, X, W):\n",
    "        best_params = result.x\n",
    "        best_score = result.fun  # Negate to get the actual score\n",
    "\n",
    "        C = best_params[0]\n",
    "        # gamma = best_params[1]\n",
    "        #class_weight = {1: best_params[1], -1: best_params[2]}\n",
    "\n",
    "        best_model = SVC(C=C, kernel=lambda X, Y: bessel_kernel_matrix(X, Y, sigma=1.0, v=1.0))\n",
    "        best_model.fit(X, W)\n",
    "        support_indices = best_model.support_\n",
    "        alpha = np.abs(best_model.dual_coef_[0])\n",
    "        ATE = estimate_ate(Y, W, X, alpha, support_indices)\n",
    "        DIM = estimate_diff(Y, W, X, alpha, support_indices)\n",
    "        ESS = estimate_ess(Y, W, X, alpha, support_indices)\n",
    "        STD = compute_wnayman(Y, W, X, alpha, support_indices)\n",
    "        \n",
    "        print(\"Best Hyperparameters:\", best_params)\n",
    "        print(\"Best Cross-Validation Score:\", best_score)\n",
    "        print(\"ATE:\", ATE)\n",
    "        print(\"STD:\", STD)\n",
    "\n",
    "        return(ATE, DIM, ESS, C, STD)\n",
    "    \n",
    "    # Initialize the Bayesian Optimization\n",
    "    n_initial_points = 5\n",
    "    n_calls = 50\n",
    "\n",
    "    # 定义综合停止器\n",
    "    stopper_1 = BOStopper(\n",
    "        conv_patience=10,\n",
    "        conv_tol=1e-6,\n",
    "        # ci_width=0.2,\n",
    "        # ci_patience=10,\n",
    "        max_iter=100\n",
    "    )\n",
    "    stopper_2 = BOStopper(\n",
    "        conv_patience=10,\n",
    "        conv_tol=1e-6,\n",
    "        # ci_width=0.2,\n",
    "        # ci_patience=10,\n",
    "        max_iter=100\n",
    "    )\n",
    "\n",
    "    # rf = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "    Y_train, Y_test, X_train, X_test, W_train, W_test = train_test_split(Y, X, W, test_size=0.5, random_state=42)\n",
    "\n",
    "    result_1 = gp_minimize(objective_1, dimensions, n_initial_points=n_initial_points, callback=[stopper_1], random_state=42, verbose=True)\n",
    "    result_1 = crossfitting(result_1, Y_test, X_test, W_test)\n",
    "\n",
    "    result_2 = gp_minimize(objective_2, dimensions, n_initial_points=n_initial_points, callback=[stopper_2], random_state=42, verbose=True)\n",
    "    result_2 = crossfitting(result_2, Y_train, X_train, W_train)\n",
    "\n",
    "    ATE = (result_1[0] + result_2[0])/2\n",
    "    DIM = (result_1[1] + result_2[1])/2\n",
    "    ESS = result_1[2] + result_2[2]\n",
    "    C = (result_1[3] + result_2[3])/2\n",
    "    STD = (result_1[4] + result_2[4])/2\n",
    "\n",
    "    return(ATE, DIM, ESS, C, STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset and hyperparameter space\n",
    "num_datasets = 1\n",
    "simulation_results_A = {n: [] for n in range(num_datasets)}\n",
    "\n",
    "\n",
    "kernels = ['bessel']\n",
    "# pi_range = np.linspace(0.0, 1.0, 11)\n",
    "\n",
    "# pi_range = [0.0]\n",
    "# results = {pi: [] for pi in pi_range}\n",
    "\n",
    "c_uppers = [1e+3]\n",
    "\n",
    "results = {c_upper: [] for c_upper in c_uppers}\n",
    "\n",
    "\n",
    "for n in range(num_datasets):\n",
    "    # simulate_data = pd.read_csv('../data_A_2000/data_scenario_G_n_2000_dataset_{}.csv'.format(n))\n",
    "    simulate_data = pd.read_csv('../data_B2_2000/dataset_{}.csv'.format(n+3))\n",
    "\n",
    "    X = simulate_data.drop(columns=['T', 'Y'])\n",
    "    T = simulate_data['T']\n",
    "    Y = simulate_data['Y']\n",
    "\n",
    "    W = 2 * T - 1\n",
    "\n",
    "    for kernel in kernels:\n",
    "\n",
    "        for c_upper in c_uppers:\n",
    "\n",
    "            result = bayesian_opt(Y, X, W, kernel, c_upper) \n",
    "\n",
    "            results[c_upper].append(result)\n",
    "\n",
    "\n",
    "        data = results\n",
    "\n",
    "        data_str = json.dumps(data, cls=NumpyEncoder, indent=4)\n",
    "\n",
    "\n",
    "        # with open(f'../results/{kernel}/simu_B2_100_2000_f.json', 'w') as f:\n",
    "        #     f.write(data_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1000.0: [(11.025426462036265, 0.38620528593358117, 902.3898474559594, 0.0007558493634133259, 1.3234235486867854)]}\n",
      "c_upper: 1e+3, Mean: 11.025426462036265, Variance: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rmaat\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\fromnumeric.py:3787: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\Users\\rmaat\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "print(results)\n",
    "c_upper = []\n",
    "kernel_ate_estimates = []\n",
    "for c_uppers, models in results.items():\n",
    "    for model in models:\n",
    "        c_upper.append(float(c_uppers))\n",
    "        kernel_ate_estimates.append(float(model[0]))\n",
    "\n",
    "data = pd.DataFrame({'c_upper': c_upper, 'ATE': kernel_ate_estimates})\n",
    "\n",
    "# 分组\n",
    "grouped_data = data.groupby('c_upper')['ATE'].apply(list).tolist()\n",
    "\n",
    "# 计算每个分组的平均值和方差\n",
    "grouped_stats = []\n",
    "for group in grouped_data:\n",
    "    mean = np.mean(group)\n",
    "    variance = np.var(group, ddof=1)  # 使用样本方差\n",
    "    grouped_stats.append({'mean': mean, 'variance': variance})\n",
    "\n",
    "# 打印结果\n",
    "for i, stats in enumerate(grouped_stats):\n",
    "    print(f\"c_upper: 1e+{i+3}, Mean: {stats['mean']}, Variance: {stats['variance']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ate(results):\n",
    "\n",
    "    kernel = 'rbf'\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3,figsize=(12, 4))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.3)\n",
    "\n",
    "    i= 0\n",
    "\n",
    "    # 过滤出当前kernel的所有模型\n",
    "    i = i  \n",
    "    c_upper = []\n",
    "    kernel_ate_estimates = []\n",
    "    for c_uppers, models in results.items():\n",
    "        for model in models:\n",
    "            c_upper.append(float(c_uppers))\n",
    "            kernel_ate_estimates.append(float(model[0]))\n",
    "\n",
    "    data = pd.DataFrame({'c_upper': c_upper, 'ATE': kernel_ate_estimates})\n",
    "    grouped_data = data.groupby('c_upper')['ATE'].apply(list).tolist()\n",
    "    #print(sorted(set(pi)))\n",
    "    print(grouped_data)\n",
    "\n",
    "    # x_sticks = pi\n",
    "    x_values = [f\"$10^{{{int(np.log10(c))} }}$\" for c in sorted(set(c_upper))]\n",
    "    print(x_values)\n",
    "    # 绘制箱型图\n",
    "    axes[i].boxplot(grouped_data, whis=(5, 95), showfliers=False)\n",
    "    axes[i].axhline(y=-0.4, color='r', linestyle='--', label='True ATE (A)')\n",
    "    # axes[i].axhline(y=10, color='r', linestyle='--', label='True ATE (B)')\n",
    "    #axes[i].set_xscale('log')\n",
    "    axes[i].set_title(f'{kernel}')\n",
    "    #axes[i].set_xticks(x_sticks)  # 保持刻度位置不变\n",
    "    axes[i].set_xticklabels(x_values)\n",
    "    axes[i].set_xlabel('c_upper')\n",
    "    axes[i].set_ylabel('ATE')\n",
    "    axes[i].legend()\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    i = i  \n",
    "    c_upper = []\n",
    "    ESS = []\n",
    "    for c_uppers, models in results.items():\n",
    "        for model in models:\n",
    "            c_upper.append(float(c_uppers))\n",
    "            ESS.append(float(model[2]))\n",
    "\n",
    "    data = pd.DataFrame({'c_upper': c_upper, 'ESS': ESS})\n",
    "    grouped_data = data.groupby('c_upper')['ESS'].apply(list).tolist()\n",
    "    #print(sorted(set(pi)))\n",
    "    print(grouped_data)\n",
    "\n",
    "    # x_sticks = pi\n",
    "    x_values = [f\"$10^{{{int(np.log10(c))} }}$\" for c in sorted(set(c_upper))]\n",
    "    # 绘制箱型图\n",
    "    axes[i].boxplot(grouped_data, whis=(5, 95), showfliers=False)\n",
    "    axes[i].axhline(y=2000, color='r', linestyle='--', label='SS')\n",
    "    #axes[i].set_xscale('log')\n",
    "    axes[i].set_title(f'{kernel}')\n",
    "    #axes[i].set_xticks(x_sticks)  # 保持刻度位置不变\n",
    "    axes[i].set_xticklabels(x_values)\n",
    "    axes[i].set_xlabel('c_upper')\n",
    "    axes[i].set_ylabel('ESS')\n",
    "    axes[i].legend()\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    # i = i  \n",
    "    # c_upper = []\n",
    "    # DIM = []\n",
    "    # for c_uppers, models in results.items():\n",
    "    #     for model in models:\n",
    "    #         c_upper.append(float(c_uppers))\n",
    "    #         DIM.append(float(model[1]))\n",
    "    \n",
    "\n",
    "    # data = pd.DataFrame({'c_upper': c_upper, 'DIM': DIM})\n",
    "    # grouped_data = data.groupby('c_upper')['DIM'].apply(list).tolist()\n",
    "    # print(grouped_data)\n",
    "\n",
    "    # x_values = [f\"$10^{{{int(np.log10(c))} }}$\" for c in sorted(set(c_upper))]\n",
    "    # # 绘制箱型图\n",
    "    # axes[i].boxplot(grouped_data)\n",
    "    # axes[i].set_title(f'{kernel}')\n",
    "    # axes[i].set_xticklabels(x_values)\n",
    "    # axes[i].set_xlabel('c_upper')\n",
    "    # axes[i].set_ylabel('DIM')\n",
    "    # axes[i].legend()\n",
    "\n",
    "    i = i\n",
    "    c_upper = []\n",
    "    STD = []\n",
    "    for c_uppers, models in results.items():\n",
    "        for model in models:\n",
    "            c_upper.append(float(c_uppers))\n",
    "            STD.append(float(model[4]))\n",
    "    \n",
    "    data = pd.DataFrame({'c_upper': c_upper, 'STD': STD})\n",
    "    grouped_data = data.groupby('c_upper')['STD'].apply(list).tolist()\n",
    "    print(grouped_data)\n",
    "\n",
    "    x_values = [f\"$10^{{{int(np.log10(c))} }}$\" for c in sorted(set(c_upper))]\n",
    "    # 绘制箱型图\n",
    "    axes[i].boxplot(grouped_data, whis=(5, 95), showfliers=False)\n",
    "    axes[i].set_title(f'{kernel}')\n",
    "    axes[i].set_xticklabels(x_values)\n",
    "    axes[i].set_xlabel('c_upper')\n",
    "    axes[i].set_ylabel('STD')\n",
    "    axes[i].legend()\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m plot_ate(\u001b[43mresults\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "plot_ate(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
